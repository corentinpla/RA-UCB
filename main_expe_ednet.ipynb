{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyKzL18lLQ-P"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtGmDB7zKMCQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from typing import Callable, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.optimize import fsolve, brentq, minimize\n",
    "from scipy.special import logsumexp, gamma as Gamma, gammainc\n",
    "from scipy.stats import weibull_min\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import RA-UCB functions from external module\n",
    "from Utils_weibull import (\n",
    "    oracle_weibull_softmax,\n",
    "    g_weibull,\n",
    "    g_inv_weibull,\n",
    "    compute_confidence_bounds_weibull_vec,\n",
    "    select_allocation,\n",
    "    update_estimates_weibull,\n",
    "    compute_regret,\n",
    "    simulate_one_round_weibull,\n",
    "    sanity_check,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EdNet KT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./pseudo_users_dataset.csv\")  \n",
    "\n",
    "# Robust boolean conversion + time cleaning\n",
    "df[\"is_correct\"] = df[\"is_correct\"].astype(str).str.lower().isin([\"true\",\"1\",\"yes\",\"y\"])\n",
    "df[\"response_time_ms\"] = pd.to_numeric(df[\"response_time_ms\"], errors=\"coerce\")\n",
    "df[\"response_time_ms\"] = df[\"response_time_ms\"] / 1000\n",
    "df = df.dropna(subset=[\"question_id\",\"response_time_ms\",\"is_correct\"])\n",
    "df = df[df[\"response_time_ms\"] > 0]\n",
    "\n",
    "res = (df.groupby(\"question_id\")\n",
    "         .apply(lambda g: pd.Series({\n",
    "             \"n\": len(g),\n",
    "             \"p_i\": g[\"is_correct\"].mean(),\n",
    "             \"K_i\": weibull_min.fit(g[\"response_time_ms\"].values, floc=0)[0],      # shape\n",
    "             \"scale_i\": weibull_min.fit(g[\"response_time_ms\"].values, floc=0)[2]  # scale (λ = 1/scale)\n",
    "         }))\n",
    "         .reset_index()\n",
    "      )\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo syntetic on 10 first arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = 10\n",
    "B = 700\n",
    "D = 1\n",
    "T = 5000\n",
    "method=\"trust-constr\"\n",
    "n_restarts=5\n",
    "\n",
    "\n",
    "p_min, p_max = 0.01, 1.0\n",
    "lambda_min, lambda_max = 1/B, 80/B\n",
    "k_min, k_max = 0.8, 2.5\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "p_true = np.array([0.3038,0.3338,0.5020,0.4550,0.2208,0.5988,0.7194,0.6264,0.6572,0.4288])\n",
    "lambda_true = np.array([1/35.804217,1/39.764151,1/43.488183,1/35.113810,1/48.429560,1/37.517613,1/36.014443,1/39.964080,1/38.084058,1/40.805778])\n",
    "k_vec = np.array([1.832726,1.902398,1.604154,1.938767,1.197973,1.614411,1.299792,1.400885,1.479928,1.536946])  \n",
    "\n",
    "print(\"True parameters (unknown to the algorithm):\")\n",
    "print(\"p_true =\", p_true)\n",
    "print(\"lambda_true =\", lambda_true)\n",
    "print(\"k_vec (connu) =\", k_vec)\n",
    "\n",
    "#Optimal allocation\n",
    "x_star = oracle_weibull_softmax(p_true, lambda_true, lambda_true, B, k_vec, n_restarts=5, seed=0)\n",
    "print(\"\\nx_star =\", x_star)\n",
    "print(\"sum(x_star) =\", np.sum(x_star))\n",
    "\n",
    "\n",
    "t1 = 0  # Reward time\n",
    "beta = 0.001\n",
    "init_steps_per_arm = int(beta * T / K)\n",
    "\n",
    "#Variable estimation\n",
    "n = np.zeros((int(T/K), K))\n",
    "Szk = np.zeros((int(T/K), K))\n",
    "theta_hat = np.zeros((int(T/K), K))\n",
    "lambda_hat = np.zeros((int(T/K), K))\n",
    "p_hat = np.zeros((int(T/K), K))\n",
    "x_est = np.zeros((int(T/K), K))\n",
    "\n",
    "# Reward Variables\n",
    "x_reward = np.zeros((T, K))\n",
    "X = np.zeros((T, K))  # Latent response times\n",
    "Y = np.zeros((T, K))  # Results (correct/incorrect)\n",
    "F = np.zeros((T, K))\n",
    "f = np.zeros((T, K))\n",
    "f_star = np.zeros((T, K))\n",
    "reward = np.zeros(T)\n",
    "opt_reward = np.zeros(T)\n",
    "regret = np.zeros(T)\n",
    "cum_regret = np.zeros(T)\n",
    "\n",
    "phi = np.zeros((int(T/K), K))\n",
    "psi = np.zeros((int(T/K), K))\n",
    "\n",
    "# INITIALIZATION PHASE\n",
    "print(\"\\nInitialization phase...\")\n",
    "for i in range(K):\n",
    "    for step in range(init_steps_per_arm):\n",
    "        x_reward[t1 + 1] = np.zeros(K)\n",
    "        x_reward[t1 + 1, i] = B\n",
    "\n",
    "        \n",
    "        Y_sample = np.random.binomial(n=1, p=p_true)\n",
    "        U = np.random.uniform(0, 1, size=K)\n",
    "        X_sample = (-np.log(1 - U)) ** (1.0 / k_vec) / lambda_true\n",
    "        \n",
    "        X[t1 + 1] = X_sample\n",
    "        Y[t1 + 1] = Y_sample\n",
    "\n",
    "        for k in range(K):\n",
    "            if Y_sample[k] == 1 and x_reward[t1 + 1, k] >= X_sample[k]:\n",
    "                f[t1 + 1, k] = 1\n",
    "                F[t1 + 1, k] = X_sample[k]\n",
    "            else:\n",
    "                f[t1 + 1, k] = 0\n",
    "                F[t1 + 1, k] = 10 * B\n",
    "                \n",
    "            if Y_sample[k] == 1 and x_star[k] >= X_sample[k]:\n",
    "                f_star[t1 + 1, k] = 1\n",
    "            else:\n",
    "                f_star[t1 + 1, k] = 0\n",
    "\n",
    "        phi[t1 // K + 1, i] = f[t1 + 1, i]\n",
    "        psi[t1 // K + 1, i] = F[t1 + 1, i]\n",
    "        update_estimates_weibull(t1 // K, i, k_vec[i], phi, psi, n, Szk, theta_hat, lambda_hat, p_hat, x_est, B)\n",
    "        compute_regret(t1, reward, opt_reward, regret, cum_regret, f, f_star)\n",
    "        t1 += 1\n",
    "\n",
    "# MAIN PHASE \n",
    "print(\"Main phase...\")\n",
    "total_iterations = (T // K - 1 - t1 // K) * K\n",
    "with tqdm(total=total_iterations, desc=\"Progression\") as pbar:\n",
    "    for t in range(t1 // K, T // K - 1):  # Estimation time\n",
    "        for i in range(K):\n",
    "            \n",
    "            Y_sample = np.random.binomial(n=1, p=p_true)\n",
    "            U = np.random.uniform(0, 1, size=K)\n",
    "            X_sample = (-np.log(1 - U)) ** (1.0 / k_vec) / lambda_true\n",
    "\n",
    "            \n",
    "            p_bar, lambda_bar, lambda_bar_prime = compute_confidence_bounds_weibull_vec(\n",
    "                t, i, p_hat, lambda_hat, n, x_est, B, D, K, k_vec)\n",
    "            \n",
    "            \n",
    "            x_reward[t1 + 1] = select_allocation(lambda_bar, lambda_bar_prime, p_bar, B, k_vec)\n",
    "            x_est[t + 1, i] = x_reward[t1 + 1, i].copy()\n",
    "\n",
    "            X[t1 + 1] = X_sample\n",
    "            Y[t1 + 1] = Y_sample\n",
    "\n",
    "            for k in range(K):\n",
    "                if Y_sample[k] == 1 and x_reward[t1 + 1, k] >= X_sample[k]:\n",
    "                    f[t1 + 1, k] = 1\n",
    "                    F[t1 + 1, k] = X_sample[k]\n",
    "                else:\n",
    "                    f[t1 + 1, k] = 0\n",
    "                    F[t1 + 1, k] = 10 * B\n",
    "                    \n",
    "                if Y_sample[k] == 1 and x_star[k] >= X_sample[k]:\n",
    "                    f_star[t1 + 1, k] = 1\n",
    "                else:\n",
    "                    f_star[t1 + 1, k] = 0\n",
    "\n",
    "            phi[t + 1, i] = f[t1 + 1, i]\n",
    "            psi[t + 1, i] = F[t1 + 1, i]\n",
    "            update_estimates_weibull(t, i, k_vec[i], phi, psi, n, Szk, theta_hat, lambda_hat, p_hat, x_est, B)\n",
    "            compute_regret(t1, reward, opt_reward, regret, cum_regret, f, f_star)\n",
    "\n",
    "            t1 += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "print(f\"\\nSimulation complete!\")\n",
    "\n",
    "actual_cum_regret = np.sum(regret[1:t1+1])\n",
    "print(f\"Final cumulative regret (correct): {actual_cum_regret:.2f}\")\n",
    "print(f\"Last index t1 = {t1}\")\n",
    "\n",
    "print(\"\\n=== DEBUG INFO ===\")\n",
    "print(f\"Total reward: {np.sum(reward[1:t1+1]):.2f}\")\n",
    "print(f\"Total opt_reward: {np.sum(opt_reward[1:t1+1]):.2f}\")\n",
    "print(f\"Mean instantaneous regret: {np.mean(regret[1:t1+1]):.4f}\")\n",
    "print(f\"Std instantaneous regret: {np.std(regret[1:t1+1]):.4f}\")\n",
    "print(f\"Min/Max instantaneous regret: {np.min(regret[1:t1+1]):.2f} / {np.max(regret[1:t1+1]):.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== ALLOCATION COMPARISON (last round) ===\")\n",
    "print(f\"x_star = {x_star}\")\n",
    "print(f\"x_reward (last valid) = {x_reward[t1]}\")\n",
    "print(f\"Allocation error (L2): {np.linalg.norm(x_reward[t1] - x_star):.4f}\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "\n",
    "cum_regret_correct = np.cumsum(regret[1:t1+1])\n",
    "\n",
    "# Cumulative regret\n",
    "ax1 = axes[0, 0]\n",
    "t_axis = np.arange(1, t1+1)\n",
    "ax1.plot(t_axis, cum_regret_correct, linewidth=2, label='Cumulative regret')\n",
    "ax1.set_xlabel('t')\n",
    "ax1.set_ylabel('Cumulative regret')\n",
    "ax1.set_title('Cumulative regret of RA-UCB Weibull')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2) instant regret\n",
    "ax2 = axes[0, 1]\n",
    "window = 100\n",
    "regret_smooth = np.convolve(regret[1:t1+1], np.ones(window)/window, mode='valid')\n",
    "ax2.plot(np.arange(len(regret_smooth)), regret_smooth, linewidth=1, label=f'Instantaneous regret (MA {window})')\n",
    "ax2.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('t')\n",
    "ax2.set_ylabel('Regret instantané')\n",
    "ax2.set_title('Instantaneous regret (moving average)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3) per round reward\n",
    "ax3 = axes[1, 0]\n",
    "reward_smooth = np.convolve(reward[1:t1+1], np.ones(window)/window, mode='valid')\n",
    "opt_reward_smooth = np.convolve(opt_reward[1:t1+1], np.ones(window)/window, mode='valid')\n",
    "ax3.plot(np.arange(len(reward_smooth)), reward_smooth, label='Reward (algorithm)')\n",
    "ax3.plot(np.arange(len(opt_reward_smooth)), opt_reward_smooth, label='Reward (oracle)', linestyle='--')\n",
    "ax3.set_xlabel('t')\n",
    "ax3.set_ylabel('Reward')\n",
    "ax3.set_title('Mean reward per round')\n",
    "ax3.legend()\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "arm_to_track = 1 \n",
    "ax4.plot(np.arange(1, t1+1), x_reward[1:t1+1, arm_to_track], label=f'x_reward[arm {arm_to_track}]', alpha=0.7)\n",
    "ax4.axhline(x_star[arm_to_track], color='r', linestyle='--', label=f'x_star[arm {arm_to_track}]')\n",
    "ax4.set_xlabel('t')\n",
    "ax4.set_ylabel('Allocation')\n",
    "ax4.set_title(f'Allocation to arm {arm_to_track} vs optimal')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On real data : user revealed sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RA-UCB Simulation with REAL observations from EdNet KT3 =====\n",
    "\n",
    "# Load the pseudo-users dataset\n",
    "pseudo_df = pd.read_csv(\"./pseudo_users_dataset.csv\")\n",
    "\n",
    "# Get list of questions (arms)\n",
    "questions = res[\"question_id\"].values\n",
    "K = len(questions)\n",
    "\n",
    "# Ground truth parameters (estimated offline) - in SECONDS\n",
    "p_true = res[\"p_i\"].values\n",
    "k_vec = res[\"K_i\"].values  # k connu (comme dans le code simulé)\n",
    "scale_ms = res[\"scale_i\"].values  \n",
    "scale_s = scale_ms / 10000  # scale en daizaine secondes\n",
    "lambda_true = 1.0 / scale_s  # λ = 1/scale\n",
    "\n",
    "# Prepare real observations: organize by question\n",
    "users = pseudo_df[\"pseudo_user_id\"].unique()\n",
    "n_users = len(users)\n",
    "\n",
    "obs_by_question = {}\n",
    "for q in questions:\n",
    "    q_data = pseudo_df[pseudo_df[\"question_id\"] == q].copy()\n",
    "    q_data = q_data.set_index(\"pseudo_user_id\")\n",
    "    obs_list = []\n",
    "    for u in users:\n",
    "        row = q_data.loc[u]\n",
    "        obs_list.append((row[\"response_time_ms\"] / 1000, row[\"is_correct\"]))  # Convertir en secondes\n",
    "    obs_by_question[q] = obs_list\n",
    "\n",
    "# Compute mean budget (average total time per user on 20 questions)\n",
    "B_all = np.array([sum(obs_by_question[q][u][0] for q in questions) for u in range(n_users)])\n",
    "B = B_all.mean()  # Budget FIXE = temps moyen\n",
    "\n",
    "# ===== PARAMETRES DE SIMULATION =====\n",
    "D = 1\n",
    "T = 1000  # Rounds per experiment\n",
    "num_experiments = 1  # 5 experiments with different batches\n",
    "method = \"trust-constr\"\n",
    "n_restarts = 5\n",
    "# Init phase: at least some rounds per arm for initial estimates\n",
    "# With T=1000 and K=20, use ~5% of budget for init = 50 rounds = 2-3 per arm\n",
    "init_steps_per_arm = max(2, int(0.05 * T / K))  # ~2-3 steps par bras\n",
    "\n",
    "# Uniform allocation (baseline)\n",
    "x_uniform = np.ones(K) * B / K\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RA-UCB avec observations RÉELLES (EdNet KT3)\")\n",
    "print(f\"5 experiments × T={T} users each\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"K = {K} bras (questions)\")\n",
    "print(f\"B = {B:.2f} s (budget fixe = temps moyen)\")\n",
    "print(f\"x_uniform = B/K = {B/K:.2f} s par bras\")\n",
    "print(f\"D = {D}\")\n",
    "print(f\"init_steps_per_arm = {init_steps_per_arm} (init rounds = {init_steps_per_arm * K})\")\n",
    "print(f\"n_users disponibles = {n_users}\")\n",
    "\n",
    "print(\"\\nTrue parameters (estimated offline):\")\n",
    "print(\"p_true =\", p_true)\n",
    "print(\"lambda_true =\", lambda_true)\n",
    "print(\"k_vec (connu) =\", k_vec)\n",
    "\n",
    "# Optimal allocation (oracle knows all)\n",
    "x_star = oracle_weibull_softmax(p_true, lambda_true, lambda_true, B, k_vec, n_restarts=5, seed=0)\n",
    "print(\"\\nx_star =\", x_star)\n",
    "print(\"sum(x_star) =\", np.sum(x_star))\n",
    "\n",
    "# Expected reward with uniform allocation\n",
    "reward_uniform_expected = np.sum(p_true * (1 - np.exp(-(lambda_true * x_uniform) ** k_vec)))\n",
    "reward_opt_expected = np.sum(p_true * (1 - np.exp(-(lambda_true * x_star) ** k_vec)))\n",
    "print(f\"\\nReward espéré oracle: {reward_opt_expected:.4f}\")\n",
    "print(f\"Reward espéré uniforme: {reward_uniform_expected:.4f}\")\n",
    "print(f\"Regret uniforme par round: {reward_opt_expected - reward_uniform_expected:.4f}\")\n",
    "\n",
    "# Storage for the 5 experiments\n",
    "all_regrets_ucb = []\n",
    "all_regrets_uniform = []\n",
    "\n",
    "# Loop over experiments\n",
    "for exp in range(num_experiments):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Experiment {exp+1}/{num_experiments}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Offset pour utiliser un batch différent de users\n",
    "    user_offset = exp * T\n",
    "    if user_offset + T > n_users:\n",
    "        print(f\"WARNING: Not enough users for experiment {exp+1}\")\n",
    "        break\n",
    "    \n",
    "    t1 = 0\n",
    "    user_idx = user_offset\n",
    "\n",
    "    # Estimation Variables\n",
    "    n = np.zeros((int(T/K), K))\n",
    "    Szk = np.zeros((int(T/K), K))\n",
    "    theta_hat = np.zeros((int(T/K), K))\n",
    "    lambda_hat = np.zeros((int(T/K), K))\n",
    "    p_hat = np.zeros((int(T/K), K))\n",
    "    x_est = np.zeros((int(T/K), K))\n",
    "\n",
    "    # Reward Variables - UCB\n",
    "    x_reward = np.zeros((T, K))\n",
    "    X = np.zeros((T, K))\n",
    "    Y = np.zeros((T, K))\n",
    "    F = np.zeros((T, K))\n",
    "    f = np.zeros((T, K))\n",
    "    f_star = np.zeros((T, K))\n",
    "    reward = np.zeros(T)\n",
    "    opt_reward = np.zeros(T)\n",
    "    regret = np.zeros(T)\n",
    "    cum_regret = np.zeros(T)\n",
    "    \n",
    "    # Reward Variables - Uniform baseline\n",
    "    f_uniform = np.zeros((T, K))\n",
    "    reward_uniform = np.zeros(T)\n",
    "    regret_uniform = np.zeros(T)\n",
    "\n",
    "    phi = np.zeros((int(T/K), K))\n",
    "    psi = np.zeros((int(T/K), K))\n",
    "\n",
    "    # INITIALIZATION PHASE\n",
    "    for i in range(K):\n",
    "        for step in range(init_steps_per_arm):\n",
    "            if user_idx >= user_offset + T:\n",
    "                break\n",
    "                \n",
    "            x_reward[t1 + 1] = np.zeros(K)\n",
    "            x_reward[t1 + 1, i] = B\n",
    "            \n",
    "            # Observation RÉELLE\n",
    "            Y_sample = np.zeros(K)\n",
    "            X_sample = np.zeros(K)\n",
    "            for k in range(K):\n",
    "                X_sample[k], is_correct = obs_by_question[questions[k]][user_idx]\n",
    "                Y_sample[k] = 1 if is_correct else 0\n",
    "            \n",
    "            X[t1 + 1] = X_sample\n",
    "            Y[t1 + 1] = Y_sample\n",
    "            \n",
    "            # UCB reward\n",
    "            for k in range(K):\n",
    "                if Y_sample[k] == 1 and x_reward[t1 + 1, k] >= X_sample[k]:\n",
    "                    f[t1 + 1, k] = 1\n",
    "                    F[t1 + 1, k] = X_sample[k]\n",
    "                else:\n",
    "                    f[t1 + 1, k] = 0\n",
    "                    F[t1 + 1, k] = 10 * B\n",
    "                \n",
    "                # Oracle reward\n",
    "                if Y_sample[k] == 1 and x_star[k] >= X_sample[k]:\n",
    "                    f_star[t1 + 1, k] = 1\n",
    "                else:\n",
    "                    f_star[t1 + 1, k] = 0\n",
    "                    \n",
    "                # Uniform reward\n",
    "                if Y_sample[k] == 1 and x_uniform[k] >= X_sample[k]:\n",
    "                    f_uniform[t1 + 1, k] = 1\n",
    "                else:\n",
    "                    f_uniform[t1 + 1, k] = 0\n",
    "            \n",
    "            phi[t1 // K + 1, i] = f[t1 + 1, i]\n",
    "            psi[t1 // K + 1, i] = F[t1 + 1, i]\n",
    "            update_estimates_weibull(t1 // K, i, k_vec[i], phi, psi, n, Szk, theta_hat, lambda_hat, p_hat, x_est, B)\n",
    "            compute_regret(t1, reward, opt_reward, regret, cum_regret, f, f_star)\n",
    "            \n",
    "            # Uniform regret\n",
    "            reward_uniform[t1 + 1] = np.sum(f_uniform[t1 + 1])\n",
    "            regret_uniform[t1 + 1] = opt_reward[t1 + 1] - reward_uniform[t1 + 1]\n",
    "            \n",
    "            t1 += 1\n",
    "            user_idx += 1\n",
    "\n",
    "    # MAIN PHASE\n",
    "    total_iterations = min((T // K - 1 - t1 // K) * K, T - t1)\n",
    "    for t in range(t1 // K, T // K - 1):\n",
    "        for i in range(K):\n",
    "            if user_idx >= user_offset + T:\n",
    "                break\n",
    "            \n",
    "            # Observation RÉELLE\n",
    "            Y_sample = np.zeros(K)\n",
    "            X_sample = np.zeros(K)\n",
    "            for k in range(K):\n",
    "                X_sample[k], is_correct = obs_by_question[questions[k]][user_idx]\n",
    "                Y_sample[k] = 1 if is_correct else 0\n",
    "            \n",
    "            # Calcul des bornes de confiance\n",
    "            p_bar, lambda_bar, lambda_bar_prime = compute_confidence_bounds_weibull_vec(\n",
    "                t, i, p_hat, lambda_hat, n, x_est, B, D, K, k_vec)\n",
    "            \n",
    "            # Sélection UCB\n",
    "            x_reward[t1 + 1] = select_allocation(lambda_bar, lambda_bar_prime, p_bar, B, k_vec)\n",
    "            x_est[t + 1, i] = x_reward[t1 + 1, i].copy()\n",
    "            \n",
    "            X[t1 + 1] = X_sample\n",
    "            Y[t1 + 1] = Y_sample\n",
    "            \n",
    "            for k in range(K):\n",
    "                # UCB reward\n",
    "                if Y_sample[k] == 1 and x_reward[t1 + 1, k] >= X_sample[k]:\n",
    "                    f[t1 + 1, k] = 1\n",
    "                    F[t1 + 1, k] = X_sample[k]\n",
    "                else:\n",
    "                    f[t1 + 1, k] = 0\n",
    "                    F[t1 + 1, k] = 10 * B\n",
    "                \n",
    "                # Oracle reward\n",
    "                if Y_sample[k] == 1 and x_star[k] >= X_sample[k]:\n",
    "                    f_star[t1 + 1, k] = 1\n",
    "                else:\n",
    "                    f_star[t1 + 1, k] = 0\n",
    "                    \n",
    "                # Uniform reward\n",
    "                if Y_sample[k] == 1 and x_uniform[k] >= X_sample[k]:\n",
    "                    f_uniform[t1 + 1, k] = 1\n",
    "                else:\n",
    "                    f_uniform[t1 + 1, k] = 0\n",
    "            \n",
    "            phi[t + 1, i] = f[t1 + 1, i]\n",
    "            psi[t + 1, i] = F[t1 + 1, i]\n",
    "            update_estimates_weibull(t, i, k_vec[i], phi, psi, n, Szk, theta_hat, lambda_hat, p_hat, x_est, B)\n",
    "            compute_regret(t1, reward, opt_reward, regret, cum_regret, f, f_star)\n",
    "            \n",
    "            # Uniform regret\n",
    "            reward_uniform[t1 + 1] = np.sum(f_uniform[t1 + 1])\n",
    "            regret_uniform[t1 + 1] = opt_reward[t1 + 1] - reward_uniform[t1 + 1]\n",
    "            \n",
    "            t1 += 1\n",
    "            user_idx += 1\n",
    "        \n",
    "        if user_idx >= user_offset + T:\n",
    "            break\n",
    "    \n",
    "    # Store results for this experiment\n",
    "    cum_regret_ucb = np.cumsum(regret[1:t1+1])\n",
    "    cum_regret_unif = np.cumsum(regret_uniform[1:t1+1])\n",
    "    all_regrets_ucb.append(cum_regret_ucb)\n",
    "    all_regrets_uniform.append(cum_regret_unif)\n",
    "    \n",
    "    print(f\"  UCB regret final: {cum_regret_ucb[-1]:.2f}\")\n",
    "    print(f\"  Uniform regret final: {cum_regret_unif[-1]:.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ===== COMPUTE STATISTICS ACROSS EXPERIMENTS =====\n",
    "# Pad arrays to same length (in case some experiments ended earlier)\n",
    "max_len = max(len(r) for r in all_regrets_ucb)\n",
    "padded_ucb = np.zeros((num_experiments, max_len))\n",
    "padded_uniform = np.zeros((num_experiments, max_len))\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    padded_ucb[i, :len(all_regrets_ucb[i])] = all_regrets_ucb[i]\n",
    "    padded_ucb[i, len(all_regrets_ucb[i]):] = all_regrets_ucb[i][-1]\n",
    "    padded_uniform[i, :len(all_regrets_uniform[i])] = all_regrets_uniform[i]\n",
    "    padded_uniform[i, len(all_regrets_uniform[i]):] = all_regrets_uniform[i][-1]\n",
    "\n",
    "# Compute mean and confidence intervals (95%)\n",
    "mean_ucb = np.mean(padded_ucb, axis=0)\n",
    "std_ucb = np.std(padded_ucb, axis=0)\n",
    "ci_ucb = 1.96 * std_ucb / np.sqrt(num_experiments)\n",
    "\n",
    "mean_uniform = np.mean(padded_uniform, axis=0)\n",
    "std_uniform = np.std(padded_uniform, axis=0)\n",
    "ci_uniform = 1.96 * std_uniform / np.sqrt(num_experiments)\n",
    "\n",
    "# ===== PLOT WITH CONFIDENCE INTERVALS =====\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "t_axis = np.arange(1, max_len + 1)\n",
    "\n",
    "# Plot UCB\n",
    "ax.plot(t_axis, mean_ucb, 'b-', linewidth=2, label='RA-UCB')\n",
    "ax.fill_between(t_axis, mean_ucb - ci_ucb, mean_ucb + ci_ucb, alpha=0.3, color='blue')\n",
    "\n",
    "# Plot Uniform\n",
    "ax.plot(t_axis, mean_uniform, 'r-', linewidth=2, label='Uniform allocation')\n",
    "ax.fill_between(t_axis, mean_uniform - ci_uniform, mean_uniform + ci_uniform, alpha=0.3, color='red')\n",
    "\n",
    "ax.set_xlabel('Round t', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Regret', fontsize=14)\n",
    "ax.set_title(f'RA-UCB vs Uniform Allocation (Real Data - {num_experiments} experiments)', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to PDF\n",
    "pdf_path = \"./RA_UCB_real_data_results.pdf\"\n",
    "plt.savefig(pdf_path, format='pdf', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nFigure saved to: {pdf_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ===== SUMMARY STATISTICS =====\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Summary ({num_experiments} experiments)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nRA-UCB:\")\n",
    "print(f\"  Final regret: {mean_ucb[-1]:.2f} ± {ci_ucb[-1]:.2f}\")\n",
    "print(f\"  Std: {std_ucb[-1]:.2f}\")\n",
    "\n",
    "print(f\"\\nUniform allocation:\")\n",
    "print(f\"  Final regret: {mean_uniform[-1]:.2f} ± {ci_uniform[-1]:.2f}\")\n",
    "print(f\"  Std: {std_uniform[-1]:.2f}\")\n",
    "\n",
    "print(f\"\\nImprovement of UCB over Uniform: {(1 - mean_ucb[-1]/mean_uniform[-1])*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
