{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b92001bb",
      "metadata": {},
      "source": [
        "# RA-UCB for Discrete Weibull - Criteo Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2b49d484",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import weibull_min\n",
        "\n",
        "# Import Discrete Weibull RA-UCB functions\n",
        "from Utils_weibull_discrete import (\n",
        "    oracle_discrete_weibull,\n",
        "    discrete_weibull_pmf,\n",
        "    discrete_weibull_cdf,\n",
        "    discrete_weibull_sample,\n",
        "    compute_confidence_bounds_discrete_weibull_vec,\n",
        "    select_allocation_discrete,\n",
        "    update_estimates_discrete_weibull,\n",
        "    compute_regret,\n",
        "    simulate_one_round_discrete_weibull,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9293eb",
      "metadata": {},
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5da4f3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the exposures before click dataset (created from Criteo)\n",
        "DATA_PATH = \"./data/criteo_exposures_before_click.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(df.head())\n",
        "\n",
        "# Select top K campaigns\n",
        "K = 15\n",
        "top_campaigns = df['campaign'].value_counts().head(K).index.tolist()\n",
        "df_filtered = df[df['campaign'].isin(top_campaigns)].copy()\n",
        "\n",
        "print(f\"\\nUsing top {K} campaigns: {top_campaigns}\")\n",
        "print(f\"Filtered dataset size: {len(df_filtered)}\")\n",
        "\n",
        "# Estimate Discrete Weibull parameters for each campaign (arm)\n",
        "# For discrete Weibull: P(X = k) = q^(k^β) - q^((k+1)^β)\n",
        "# We estimate β (shape) and q (scale) from the observed data\n",
        "\n",
        "def fit_discrete_weibull(data, beta_fixed=None):\n",
        "    \"\"\"\n",
        "    Fit discrete Weibull parameters to data.\n",
        "    If beta_fixed is provided, only estimate q.\n",
        "    Uses method of moments / MLE approximation.\n",
        "    \"\"\"\n",
        "    data = np.asarray(data)\n",
        "    mean_obs = np.mean(data)\n",
        "    var_obs = np.var(data)\n",
        "    \n",
        "    if beta_fixed is not None:\n",
        "        beta = beta_fixed\n",
        "    else:\n",
        "        # Estimate beta from coefficient of variation\n",
        "        # For discrete Weibull, higher beta = more concentrated\n",
        "        cv = np.std(data) / (mean_obs + 1e-6)\n",
        "        beta = max(0.5, min(3.0, 1.5 / (cv + 0.1)))\n",
        "    \n",
        "    # Estimate q using mean\n",
        "    # E[X] ≈ Σ q^((k+1)^β) for small q\n",
        "    # Approximate: q ≈ exp(-1/mean) for beta=1\n",
        "    # More generally, use numerical search\n",
        "    from scipy.optimize import minimize_scalar\n",
        "    \n",
        "    def neg_likelihood(q):\n",
        "        if q <= 0.01 or q >= 0.99:\n",
        "            return 1e10\n",
        "        # Compute expected mean for this q\n",
        "        expected_mean = 0\n",
        "        for k in range(int(max(data)) + 10):\n",
        "            expected_mean += q ** ((k + 1) ** beta)\n",
        "            if q ** ((k + 1) ** beta) < 1e-10:\n",
        "                break\n",
        "        return (expected_mean - mean_obs) ** 2\n",
        "    \n",
        "    result = minimize_scalar(neg_likelihood, bounds=(0.01, 0.99), method='bounded')\n",
        "    q = result.x\n",
        "    \n",
        "    return q, beta\n",
        "\n",
        "# Estimate parameters for each campaign\n",
        "campaign_params = {}\n",
        "for campaign in top_campaigns:\n",
        "    campaign_data = df_filtered[df_filtered['campaign'] == campaign]\n",
        "    exposures = campaign_data['exposures_before_click'].values\n",
        "    p_click = campaign_data['clicked'].mean()\n",
        "    \n",
        "    # Fit discrete Weibull to the exposure counts\n",
        "    q_est, beta_est = fit_discrete_weibull(exposures, beta_fixed=1.5)  # Fix beta for now\n",
        "    \n",
        "    campaign_params[campaign] = {\n",
        "        'p': p_click,\n",
        "        'q': q_est,\n",
        "        'beta': beta_est,\n",
        "        'n_samples': len(campaign_data),\n",
        "        'mean_exposures': np.mean(exposures),\n",
        "    }\n",
        "\n",
        "# Create parameter arrays\n",
        "campaign_list = top_campaigns\n",
        "p_true = np.array([campaign_params[c]['p'] for c in campaign_list])\n",
        "q_true = np.array([campaign_params[c]['q'] for c in campaign_list])\n",
        "beta_vec = np.array([campaign_params[c]['beta'] for c in campaign_list])\n",
        "\n",
        "print(\"\\n=== Estimated Parameters ===\")\n",
        "params_df = pd.DataFrame(campaign_params).T\n",
        "print(params_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f5e857",
      "metadata": {},
      "source": [
        "# Single Experiment - RA-UCB with Discrete Weibull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5fc75f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulation parameters\n",
        "T = 2000  # Total rounds\n",
        "B = 50    # Total budget (number of exposures to allocate)\n",
        "D = 1.0   # Confidence parameter\n",
        "method = \"dp\"  # Use dynamic programming for discrete optimization\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"SIMULATION: T={T}, K={K}, B={B}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nTrue parameters (unknown to algorithm):\")\n",
        "print(f\"p_true = {p_true}\")\n",
        "print(f\"q_true = {q_true}\")\n",
        "print(f\"beta_vec (known) = {beta_vec}\")\n",
        "\n",
        "# Compute optimal allocation (oracle knows everything)\n",
        "x_star = oracle_discrete_weibull(p_true, q_true, q_true, B, beta_vec, method=method)\n",
        "x_star = np.round(x_star).astype(int)  # Ensure integer allocations\n",
        "# Adjust to ensure sum = B\n",
        "diff = B - np.sum(x_star)\n",
        "if diff != 0:\n",
        "    x_star[np.argmax(x_star)] += diff\n",
        "\n",
        "print(f\"\\nx_star = {x_star}\")\n",
        "print(f\"sum(x_star) = {np.sum(x_star)}\")\n",
        "\n",
        "# Compute expected rewards\n",
        "expected_reward_opt = np.sum(p_true * (1 - q_true ** ((x_star + 1) ** beta_vec)))\n",
        "x_uniform = np.full(K, B // K)\n",
        "x_uniform[0] += B - np.sum(x_uniform)  # Adjust for rounding\n",
        "expected_reward_uniform = np.sum(p_true * (1 - q_true ** ((x_uniform + 1) ** beta_vec)))\n",
        "\n",
        "print(f\"\\nExpected reward (optimal): {expected_reward_opt:.4f}\")\n",
        "print(f\"Expected reward (uniform): {expected_reward_uniform:.4f}\")\n",
        "\n",
        "# Initialize random generator\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# SINGLE EXPERIMENT\n",
        "t1 = 0  # Reward time counter\n",
        "init_fraction = 0.001\n",
        "init_steps_per_arm = max(1, int(init_fraction * T / K))\n",
        "\n",
        "print(f\"\\ninit_steps_per_arm = {init_steps_per_arm}\")\n",
        "\n",
        "# Estimation Variables\n",
        "n_rounds = int(T / K) + 1\n",
        "n = np.zeros((n_rounds, K))\n",
        "sum_obs = np.zeros((n_rounds, K))\n",
        "q_hat = np.zeros((n_rounds, K))\n",
        "p_hat = np.zeros((n_rounds, K))\n",
        "x_est = np.zeros((n_rounds, K))\n",
        "\n",
        "# Initialize estimates\n",
        "q_hat[0, :] = 0.5\n",
        "p_hat[0, :] = 0.5\n",
        "\n",
        "# Reward Variables\n",
        "x_reward = np.zeros((T, K))\n",
        "X_obs = np.zeros((T, K))  # Observed response times (discrete)\n",
        "Y_obs = np.zeros((T, K))  # Success indicators\n",
        "F = np.zeros((T, K))      # Observed time (event or censoring)\n",
        "f = np.zeros((T, K))      # Reward indicator (1 if event observed)\n",
        "f_star = np.zeros((T, K)) # Optimal reward indicator\n",
        "reward = np.zeros(T)\n",
        "opt_reward = np.zeros(T)\n",
        "regret = np.zeros(T)\n",
        "cum_regret = np.zeros(T)\n",
        "\n",
        "phi = np.zeros((n_rounds, K))\n",
        "psi = np.zeros((n_rounds, K))\n",
        "\n",
        "# INITIALIZATION PHASE\n",
        "print(\"\\nInitialization phase...\")\n",
        "for i in range(K):\n",
        "    for step in range(init_steps_per_arm):\n",
        "        if t1 >= T - 1:\n",
        "            break\n",
        "            \n",
        "        x_reward[t1 + 1] = np.zeros(K)\n",
        "        x_reward[t1 + 1, i] = B  # Give full budget to arm i during init\n",
        "\n",
        "        # Sample from Discrete Weibull model\n",
        "        Y_sample = rng.binomial(n=1, p=p_true)\n",
        "        X_sample = np.array([discrete_weibull_sample(q_true[k], beta_vec[k], rng) for k in range(K)])\n",
        "        \n",
        "        X_obs[t1 + 1] = X_sample\n",
        "        Y_obs[t1 + 1] = Y_sample\n",
        "\n",
        "        for k in range(K):\n",
        "            # Event observed if Y=1 and allocation >= response time\n",
        "            if Y_sample[k] == 1 and x_reward[t1 + 1, k] >= X_sample[k]:\n",
        "                f[t1 + 1, k] = 1\n",
        "                F[t1 + 1, k] = X_sample[k]\n",
        "            else:\n",
        "                f[t1 + 1, k] = 0\n",
        "                F[t1 + 1, k] = x_reward[t1 + 1, k]  # Censored at allocation\n",
        "                \n",
        "            # Optimal policy reward\n",
        "            if Y_sample[k] == 1 and x_star[k] >= X_sample[k]:\n",
        "                f_star[t1 + 1, k] = 1\n",
        "            else:\n",
        "                f_star[t1 + 1, k] = 0\n",
        "\n",
        "        # Update estimates\n",
        "        est_idx = t1 // K + 1\n",
        "        if est_idx < n_rounds:\n",
        "            phi[est_idx, i] = f[t1 + 1, i]\n",
        "            psi[est_idx, i] = F[t1 + 1, i]\n",
        "            update_estimates_discrete_weibull(\n",
        "                t1 // K, i, beta_vec[i], phi, psi, n, sum_obs, q_hat, p_hat, x_est, B\n",
        "            )\n",
        "        \n",
        "        # Compute regret\n",
        "        compute_regret(t1, reward, opt_reward, regret, cum_regret, f, f_star)\n",
        "        t1 += 1\n",
        "\n",
        "# MAIN PHASE\n",
        "print(\"Main phase...\")\n",
        "total_iterations = (T // K - 1 - t1 // K) * K\n",
        "with tqdm(total=total_iterations, desc=\"Progress\") as pbar:\n",
        "    for t in range(t1 // K, T // K - 1):\n",
        "        for i in range(K):\n",
        "            if t1 >= T - 1:\n",
        "                break\n",
        "                \n",
        "            # Sample from Discrete Weibull model\n",
        "            Y_sample = rng.binomial(n=1, p=p_true)\n",
        "            X_sample = np.array([discrete_weibull_sample(q_true[k], beta_vec[k], rng) for k in range(K)])\n",
        "\n",
        "            # Compute confidence bounds\n",
        "            p_bar, q_bar, q_bar_prime = compute_confidence_bounds_discrete_weibull_vec(\n",
        "                t, i, p_hat, q_hat, n, x_est, B, D, K, beta_vec\n",
        "            )\n",
        "            \n",
        "            # Select allocation using oracle with estimated bounds\n",
        "            x_reward[t1 + 1] = select_allocation_discrete(q_bar, q_bar_prime, p_bar, B, beta_vec, method=method)\n",
        "            x_reward[t1 + 1] = np.round(x_reward[t1 + 1]).astype(int)\n",
        "            \n",
        "            # Ensure sum = B\n",
        "            diff = B - np.sum(x_reward[t1 + 1])\n",
        "            if diff != 0:\n",
        "                x_reward[t1 + 1, np.argmax(x_reward[t1 + 1])] += diff\n",
        "            \n",
        "            if t + 1 < n_rounds:\n",
        "                x_est[t + 1, i] = x_reward[t1 + 1, i]\n",
        "\n",
        "            X_obs[t1 + 1] = X_sample\n",
        "            Y_obs[t1 + 1] = Y_sample\n",
        "\n",
        "            for k in range(K):\n",
        "                if Y_sample[k] == 1 and x_reward[t1 + 1, k] >= X_sample[k]:\n",
        "                    f[t1 + 1, k] = 1\n",
        "                    F[t1 + 1, k] = X_sample[k]\n",
        "                else:\n",
        "                    f[t1 + 1, k] = 0\n",
        "                    F[t1 + 1, k] = x_reward[t1 + 1, k]\n",
        "                    \n",
        "                if Y_sample[k] == 1 and x_star[k] >= X_sample[k]:\n",
        "                    f_star[t1 + 1, k] = 1\n",
        "                else:\n",
        "                    f_star[t1 + 1, k] = 0\n",
        "\n",
        "            # Update estimates\n",
        "            if t + 1 < n_rounds:\n",
        "                phi[t + 1, i] = f[t1 + 1, i]\n",
        "                psi[t + 1, i] = F[t1 + 1, i]\n",
        "                update_estimates_discrete_weibull(\n",
        "                    t, i, beta_vec[i], phi, psi, n, sum_obs, q_hat, p_hat, x_est, B\n",
        "                )\n",
        "            \n",
        "            compute_regret(t1, reward, opt_reward, regret, cum_regret, f, f_star)\n",
        "            t1 += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "print(f\"\\nSimulation complete!\")\n",
        "actual_cum_regret = np.sum(regret[1:t1+1])\n",
        "print(f\"Final cumulative regret: {actual_cum_regret:.2f}\")\n",
        "print(f\"Last index t1 = {t1}\")\n",
        "\n",
        "# DEBUG INFO\n",
        "print(\"\\n=== DEBUG INFO ===\")\n",
        "print(f\"Total reward: {np.sum(reward[1:t1+1]):.2f}\")\n",
        "print(f\"Total opt_reward: {np.sum(opt_reward[1:t1+1]):.2f}\")\n",
        "print(f\"Mean instantaneous regret: {np.mean(regret[1:t1+1]):.4f}\")\n",
        "print(f\"Std instantaneous regret: {np.std(regret[1:t1+1]):.4f}\")\n",
        "\n",
        "# Compare final allocation with optimal\n",
        "print(\"\\n=== ALLOCATION COMPARISON (last round) ===\")\n",
        "print(f\"x_star = {x_star}\")\n",
        "print(f\"x_reward (last) = {x_reward[t1].astype(int)}\")\n",
        "print(f\"Allocation error (L1): {np.sum(np.abs(x_reward[t1] - x_star)):.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709b935c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Compute cumulative regret correctly\n",
        "cum_regret_correct = np.cumsum(regret[1:t1+1])\n",
        "t_axis = np.arange(1, t1+1)\n",
        "\n",
        "# 1) Cumulative regret\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(t_axis, cum_regret_correct, linewidth=2, label='Cumulative regret')\n",
        "ax1.set_xlabel('t')\n",
        "ax1.set_ylabel('Cumulative regret')\n",
        "ax1.set_title('Cumulative regret of RA-UCB (Discrete Weibull)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2) Instantaneous regret (moving average)\n",
        "ax2 = axes[0, 1]\n",
        "window = min(100, len(regret[1:t1+1]) // 10)\n",
        "if window > 1:\n",
        "    regret_smooth = np.convolve(regret[1:t1+1], np.ones(window)/window, mode='valid')\n",
        "    ax2.plot(np.arange(len(regret_smooth)), regret_smooth, linewidth=1, label=f'Instantaneous regret (MA {window})')\n",
        "else:\n",
        "    ax2.plot(t_axis, regret[1:t1+1], linewidth=1, label='Instantaneous regret')\n",
        "ax2.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
        "ax2.set_xlabel('t')\n",
        "ax2.set_ylabel('Instantaneous regret')\n",
        "ax2.set_title('Instantaneous regret (moving average)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3) Mean reward per round\n",
        "ax3 = axes[1, 0]\n",
        "if window > 1:\n",
        "    reward_smooth = np.convolve(reward[1:t1+1], np.ones(window)/window, mode='valid')\n",
        "    opt_reward_smooth = np.convolve(opt_reward[1:t1+1], np.ones(window)/window, mode='valid')\n",
        "    ax3.plot(np.arange(len(reward_smooth)), reward_smooth, label='Reward (algorithm)')\n",
        "    ax3.plot(np.arange(len(opt_reward_smooth)), opt_reward_smooth, label='Reward (oracle)', linestyle='--')\n",
        "else:\n",
        "    ax3.plot(t_axis, reward[1:t1+1], label='Reward (algorithm)')\n",
        "    ax3.plot(t_axis, opt_reward[1:t1+1], label='Reward (oracle)', linestyle='--')\n",
        "ax3.set_xlabel('t')\n",
        "ax3.set_ylabel('Reward')\n",
        "ax3.set_title('Mean reward per round')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4) Allocation evolution for one arm\n",
        "ax4 = axes[1, 1]\n",
        "arm_to_track = 0\n",
        "ax4.plot(t_axis, x_reward[1:t1+1, arm_to_track], label=f'x_reward[arm {arm_to_track}]', alpha=0.7)\n",
        "ax4.axhline(x_star[arm_to_track], color='r', linestyle='--', label=f'x_star[arm {arm_to_track}]')\n",
        "ax4.set_xlabel('t')\n",
        "ax4.set_ylabel('Allocation')\n",
        "ax4.set_title(f'Allocation to arm {arm_to_track} vs optimal')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
